name: Performance Regression Testing

on:
  pull_request:
    branches: [main, master]
  push:
    branches: [main, master]
  workflow_dispatch:
    inputs:
      update_baseline:
        description: 'Update performance baseline'
        required: false
        type: boolean
        default: false

# Ensure only one performance workflow runs at a time per PR
concurrency:
  group: performance-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  # Determine what files changed to decide if we need to run benchmarks
  changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.filter.outputs.performance }}
      docs_only: ${{ steps.filter.outputs.docs_only }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check for relevant changes
        uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            performance:
              - 'packages/**/*.ts'
              - 'plugins/**/*.ts'
              - 'benchmarks/**'
              - 'scripts/performance/**'
              - 'package.json'
              - 'package-lock.json'
              - 'tsconfig*.json'
              - 'tsup.config.ts'
            docs_only:
              - '**/*.md'
              - 'docs/**'
              - 'examples/**'
              - 'tutorials/**'
              - '.github/**'
              - '!.github/workflows/performance.yml'

      - name: Skip notification
        if: steps.filter.outputs.performance != 'true' && github.event_name == 'pull_request'
        run: |
          echo "### ⏭️ Performance Tests Skipped" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "No performance-relevant changes detected. Skipping benchmarks." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Performance tests run when changes are made to:" >> $GITHUB_STEP_SUMMARY
          echo "- \`packages/**/*.ts\` - Core package source" >> $GITHUB_STEP_SUMMARY
          echo "- \`plugins/**/*.ts\` - Plugin source" >> $GITHUB_STEP_SUMMARY
          echo "- \`benchmarks/**\` - Benchmark code" >> $GITHUB_STEP_SUMMARY
          echo "- \`scripts/performance/**\` - Performance scripts" >> $GITHUB_STEP_SUMMARY
          echo "- Package config files" >> $GITHUB_STEP_SUMMARY

  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    needs: changes
    # Run if: performance-relevant changes detected, OR push to main, OR manual trigger
    if: |
      needs.changes.outputs.should_run == 'true' ||
      github.event_name == 'push' ||
      github.event_name == 'workflow_dispatch'
    outputs:
      regression_detected: ${{ steps.compare.outputs.regression_detected }}

    steps:
      - name: Checkout PR code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for baseline comparison

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20.x
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Setup Turborepo cache
        uses: actions/cache@v4
        with:
          path: .turbo
          key: ${{ runner.os }}-turbo-perf-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-turbo-perf-

      - name: Build project
        run: npm run build

      # For PRs: Run benchmarks on both base and head to compare on same hardware
      - name: Get base branch
        if: github.event_name == 'pull_request'
        id: base
        run: |
          echo "ref=${{ github.event.pull_request.base.ref }}" >> $GITHUB_OUTPUT
          echo "sha=${{ github.event.pull_request.base.sha }}" >> $GITHUB_OUTPUT

      - name: Run current branch benchmarks
        id: current_benchmark
        run: |
          npm run benchmark
          if [ -f "benchmarks/result.txt" ]; then
            mkdir -p .performance/current
            cp benchmarks/result.txt .performance/current/result.txt
            echo "results_available=true" >> $GITHUB_OUTPUT
          else
            echo "results_available=false" >> $GITHUB_OUTPUT
            echo "::warning::Benchmark results file not found"
          fi

      # For PRs: checkout base branch and run benchmarks on same runner
      - name: Checkout base branch for comparison
        if: github.event_name == 'pull_request' && steps.current_benchmark.outputs.results_available == 'true'
        run: |
          # Save current benchmark results
          cp -r .performance/current /tmp/current-benchmarks

          # Checkout base branch
          git checkout ${{ steps.base.outputs.sha }}

      - name: Install base branch dependencies
        if: github.event_name == 'pull_request' && steps.current_benchmark.outputs.results_available == 'true'
        run: npm ci

      - name: Build base branch
        if: github.event_name == 'pull_request' && steps.current_benchmark.outputs.results_available == 'true'
        run: npm run build

      - name: Run base branch benchmarks
        if: github.event_name == 'pull_request' && steps.current_benchmark.outputs.results_available == 'true'
        id: base_benchmark
        run: |
          npm run benchmark
          if [ -f "benchmarks/result.txt" ]; then
            mkdir -p .performance/base
            cp benchmarks/result.txt .performance/base/result.txt
            echo "base_available=true" >> $GITHUB_OUTPUT
          else
            echo "base_available=false" >> $GITHUB_OUTPUT
            echo "::warning::Base benchmark results not available"
          fi

      - name: Restore current branch
        if: github.event_name == 'pull_request' && steps.current_benchmark.outputs.results_available == 'true'
        run: |
          # Checkout back to PR branch
          git checkout ${{ github.event.pull_request.head.sha }}

          # Restore current benchmark results
          mkdir -p .performance/current
          cp -r /tmp/current-benchmarks/* .performance/current/

          # Reinstall to get the comparison scripts
          npm ci

      - name: Compare benchmarks (PR)
        if: github.event_name == 'pull_request' && steps.current_benchmark.outputs.results_available == 'true' && steps.base_benchmark.outputs.base_available == 'true'
        id: compare
        run: |
          # Create output directory
          mkdir -p .performance/reports

          # Run comparison with base branch results as baseline
          npx ts-node scripts/performance/cli.ts \
            --check \
            --verbose \
            --results .performance/current/result.txt \
            --baseline-results .performance/base/result.txt \
            --output .performance/reports/comparison.json \
            2>&1 | tee comparison-output.txt

          # Capture exit code
          exit_code=${PIPESTATUS[0]}

          # Set outputs based on exit code
          if [ $exit_code -eq 0 ]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "message=All benchmarks passed" >> $GITHUB_OUTPUT
            echo "regression_detected=false" >> $GITHUB_OUTPUT
          elif [ $exit_code -eq 1 ]; then
            echo "status=regression" >> $GITHUB_OUTPUT
            echo "message=Performance regressions detected" >> $GITHUB_OUTPUT
            echo "regression_detected=true" >> $GITHUB_OUTPUT
          elif [ $exit_code -eq 2 ]; then
            echo "status=budget_exceeded" >> $GITHUB_OUTPUT
            echo "message=Performance budgets exceeded" >> $GITHUB_OUTPUT
            echo "regression_detected=true" >> $GITHUB_OUTPUT
          else
            echo "status=success" >> $GITHUB_OUTPUT
            echo "message=Comparison completed" >> $GITHUB_OUTPUT
            echo "regression_detected=false" >> $GITHUB_OUTPUT
          fi

          # Don't fail the step - we report in the summary
          exit 0

      - name: Run regression check against stored baseline (push to main)
        if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
        id: regression
        run: |
          # Create output directory
          mkdir -p .performance/reports

          # Run regression check against stored baseline
          npx ts-node scripts/performance/cli.ts \
            --check \
            --verbose \
            --results .performance/current/result.txt \
            --output .performance/reports/comparison.json \
            2>&1 | tee regression-output.txt

          # Don't fail on missing baseline for main branch
          exit 0

      - name: Generate GitHub summary
        if: always()
        run: |
          if [ -f ".performance/reports/comparison.md" ]; then
            cat .performance/reports/comparison.md >> $GITHUB_STEP_SUMMARY
          elif [ -f "comparison-output.txt" ]; then
            echo '## Performance Comparison Report' >> $GITHUB_STEP_SUMMARY
            echo '' >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat comparison-output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          elif [ -f "regression-output.txt" ]; then
            echo '## Performance Regression Report' >> $GITHUB_STEP_SUMMARY
            echo '' >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat regression-output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            .performance/
            comparison-output.txt
            regression-output.txt
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request' && steps.compare.outputs.status != ''
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let body = '## Performance Comparison Report\n\n';
            body += '> Benchmarks run on the **same runner** to ensure accurate comparison.\n\n';

            // Read the markdown report if available
            try {
              const reportPath = '.performance/reports/comparison.md';
              if (fs.existsSync(reportPath)) {
                body = fs.readFileSync(reportPath, 'utf8');
              } else {
                // Fallback to text output
                const outputPath = 'comparison-output.txt';
                if (fs.existsSync(outputPath)) {
                  const output = fs.readFileSync(outputPath, 'utf8');
                  body += '```\n' + output + '\n```';
                }
              }
            } catch (e) {
              body += `Status: ${{ steps.compare.outputs.status }}\n`;
              body += `Message: ${{ steps.compare.outputs.message }}`;
            }

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Performance') &&
              comment.body.includes('Report')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

  update-baseline:
    name: Update Baseline
    runs-on: ubuntu-latest
    needs: [changes, benchmark]
    if: |
      always() &&
      needs.benchmark.result == 'success' &&
      ((github.event_name == 'push' && github.ref == 'refs/heads/main') ||
       (github.event_name == 'workflow_dispatch' && github.event.inputs.update_baseline == 'true'))
    permissions:
      contents: write
      pull-requests: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20.x
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results
          path: .performance-download/

      - name: Update baseline
        env:
          GIT_REF: ${{ github.ref }}
          GIT_SHA: ${{ github.sha }}
        run: |
          # Copy current results to expected location
          mkdir -p .performance
          if [ -f ".performance-download/current/result.txt" ]; then
            cp .performance-download/current/result.txt benchmarks/result.txt
          fi

          npx ts-node scripts/performance/cli.ts \
            --update-baseline \
            --verbose

      - name: Create Pull Request for baseline update
        uses: peter-evans/create-pull-request@v7
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: 'chore: update performance baseline [skip ci]'
          title: 'chore: update performance baseline'
          body: |
            This PR updates the performance baseline after a successful benchmark run on main.

            - Auto-generated by the performance regression testing workflow
            - Commit: ${{ github.sha }}
          branch: chore/update-performance-baseline
          delete-branch: true
          add-paths: |
            .performance/baseline.json
